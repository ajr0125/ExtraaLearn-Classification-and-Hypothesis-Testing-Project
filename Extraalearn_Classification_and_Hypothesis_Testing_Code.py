# -*- coding: utf-8 -*-
"""ExtraaLearn Classification and Hypothesis Testing Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14p419soC_uXCD0ZdlPM7MbC1vZuj6G_I

# ExtraaLearn Project

## Context

The EdTech industry has been surging in the past decade immensely, and according to a forecast, the Online Education market would be worth $286.62bn by 2023 with a compound annual growth rate (CAGR) of 10.26% from 2018 to 2023. The modern era of online education has enforced a lot in its growth and expansion beyond any limit. Due to having many dominant features like ease of information sharing, personalized learning experience, transparency of assessment, etc, it is now preferable to traditional education.

In the present scenario due to the Covid-19, the online education sector has witnessed rapid growth and is attracting a lot of new customers. Due to this rapid growth, many new companies have emerged in this industry. With the availability and ease of use of digital marketing resources, companies can reach out to a wider audience with their offerings. The customers who show interest in these offerings are termed as leads. There are various sources of obtaining leads for Edtech companies, like

* The customer interacts with the marketing front on social media or other online platforms.
* The customer browses the website/app and downloads the brochure
* The customer connects through emails for more information.

The company then nurtures these leads and tries to convert them to paid customers. For this, the representative from the organization connects with the lead on call or through email to share further details.

## Objective

ExtraaLearn is an initial stage startup that offers programs on cutting-edge technologies to students and professionals to help them upskill/reskill. With a large number of leads being generated on a regular basis, one of the issues faced by ExtraaLearn is to identify which of the leads are more likely to convert so that they can allocate resources accordingly. You, as a data scientist at ExtraaLearn, have been provided the leads data to:
* Analyze and build an ML model to help identify which leads are more likely to convert to paid customers,
* Find the factors driving the lead conversion process
* Create a profile of the leads which are likely to convert


## Data Description

The data contains the different attributes of leads and their interaction details with ExtraaLearn. The detailed data dictionary is given below.


**Data Dictionary**
* ID: ID of the lead
* age: Age of the lead
* current_occupation: Current occupation of the lead. Values include 'Professional','Unemployed',and 'Student'
* first_interaction: How did the lead first interacted with ExtraaLearn. Values include 'Website', 'Mobile App'
* profile_completed: What percentage of profile has been filled by the lead on the website/mobile app. Values include Low - (0-50%), Medium - (50-75%), High (75-100%)
* website_visits: How many times has a lead visited the website
* time_spent_on_website: Total time spent on the website
* page_views_per_visit: Average number of pages on the website viewed during the visits.
* last_activity: Last interaction between the lead and ExtraaLearn.
    * Email Activity: Seeking for details about program through email, Representative shared information with lead like brochure of program , etc
    * Phone Activity: Had a Phone Conversation with representative, Had conversation over SMS with representative, etc
    * Website Activity: Interacted on live chat with representative, Updated profile on website, etc

* print_media_type1: Flag indicating whether the lead had seen the ad of ExtraaLearn in the Newspaper.
* print_media_type2: Flag indicating whether the lead had seen the ad of ExtraaLearn in the Magazine.
* digital_media: Flag indicating whether the lead had seen the ad of ExtraaLearn on the digital platforms.
* educational_channels: Flag indicating whether the lead had heard about ExtraaLearn in the education channels like online forums, discussion threads, educational websites, etc.
* referral: Flag indicating whether the lead had heard about ExtraaLearn through reference.
* status: Flag indicating whether the lead was converted to a paid customer or not.

## Importing necessary libraries and data
"""

# Data Manipulation/Reading Libraries
import pandas as pd
import numpy as np

# Library for train/test data split
from sklearn.model_selection import train_test_split

# Data visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Feature Scaling
from sklearn.preprocessing import StandardScaler

# Model Building Libraries
import statsmodels.stats.api as sms
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from statsmodels.tools.tools import add_constant
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier

# Model Optimization Libraries
from sklearn.model_selection import GridSearchCV

# Metric Score Libraries
import sklearn.metrics as metrics
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    precision_recall_curve,
    roc_curve,
    make_scorer,
)

"""## Data Overview

- Observations
- Sanity checks
"""

#Import Dataset and Make a Copy
data = pd.read_csv("ExtraaLearn.csv")
df = data.copy()

# First 5 Elements
df.head()

# Last 5 Elements
df.tail()

# Dimensions of Dataset
df.shape

"""This dataset has 4612 different leads, each with 15 different features."""

# Column Info
df.info()

"""The features of this dataset have 4 numerical variables (age, website visit, time spent on website, and page views per visit), with the rest being categorical variables."""

# Checking for Duplicate Entries in the Dataset
df.duplicated().sum()

"""There are no duplicate entries in this dataset."""

# Checking Percent of Missing Values in Each Column
pd.DataFrame(data={'% of Missing Values':round(df.isna().sum()/df.isna().count()*100,2)})

"""There are no missing values in this dataset."""

# Number of Unique Values in Each Column
df.nunique()

# Dropping ID column
df.drop(columns='ID',inplace=True)

"""Since Lead ID is unique for every value in the dataset, the column is dropped since it will not help for prediction."""

# Making a list of all catrgorical variables
cat_col=['current_occupation', 'first_interaction', 'profile_completed', 'last_activity', 'print_media_type1', 'print_media_type2', 'digital_media', 'educational_channels', 'referral']

# Printing number of count of each unique value in each column
for column in cat_col:
    print(df[column].value_counts())
    print('-'*50)

"""A lot of the categorical variables are binary, most of which are yes or no. The rest have 3 options."""

# Converting the data type of each categorical variable to 'category'
for column in cat_col:
    df[column]=df[column].astype('category')

df.info()

"""## Exploratory Data Analysis (EDA)

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Questions**
1. Leads will have different expectations from the outcome of the course and the current occupation may play a key role in getting them to participate in the program. Find out how current occupation affects lead status.
2. The company's first impression on the customer must have an impact. Do the first channels of interaction have an impact on the lead status?
3. The company uses multiple modes to interact with prospects. Which way of interaction works best?
4. The company gets leads from various channels such as print media, digital media, referrals, etc. Which of these channels have the highest lead conversion rate?
5. People browsing the website or mobile application are generally required to create a profile by sharing their personal data before they can access additional information.Does having more details about a prospect increase the chances of conversion?
"""

# Statistical Summary of the Numerical Features of the Data
df.describe()

# Defining the hist_box() function
def hist_box(data, col):
    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={'height_ratios': (0.15, 0.85)}, figsize=(12, 6))
    # Adding a graph in each part
    sns.boxplot(data=data, x=col, ax=ax_box, showmeans=True)
    sns.histplot(data=data, x=col, kde=True, ax=ax_hist)
    plt.show()

# Histogram and Boxplot for Age
hist_box(df, df.age)

"""The distribution of age for the dataset seems to be left skewed, with the boxplot indicating there are no outliers for this variable. It is also unimodal with a peak around the age of 60 and ranges from around 20 years to 60 years of age."""

# Histogram and Boxplot for Website Visits
hist_box(df, df.website_visits)

"""The distribution of website visits appears strongly right-skewed, with the boxplot indicating numerous outliers beyond about 9 website visits. It is unimodal with a peak at 2 website visits but ranges from 0 to 30 website visits."""

df[df['website_visits']>9.5]

# Histogram and Boxplot for Time Spent on Website
hist_box(df, df.time_spent_on_website)

"""The distribution of time spent on website for the dataset seems to be right skewed, with the boxplot indicating there are no outliers for this variable. It is also unimodal with a peak around 0 to 300 minutes and ranges from around 0 minutes to 2500 minutes."""

# Histogram and Boxplot for Page Views per Visit
hist_box(df, df.page_views_per_visit)

"""The distribution of page views per visit appears slightly right-skewed, with the boxplot indicating numerous outliers beyond about 6 page views per visit. It is multimodal with peaks at about 0, 2, 3, and 6 page views per visit and ranges from about 0 to 18 page views per visit."""

df[df['page_views_per_visit']>6.274]

# Countplot for Current Occupation
sns.countplot(x = df['current_occupation'])
plt.show()

df['current_occupation'].value_counts(normalize=True)

"""A majority of the leads in this dataset have a professional occupation."""

# Countplot for First Interaction
sns.countplot(x = df['first_interaction'])
plt.show()

df['first_interaction'].value_counts(normalize=True)

"""The method of first interaction for the leads in this dataset is spread relatively evenly between mobile app and website, with website having a slight majority."""

# Countplot for Profile Completion Status
sns.countplot(x = df['profile_completed'])
plt.show()

df['profile_completed'].value_counts(normalize=True)

"""Almost all of the leads (i.e. about 97%) have completed at least 50% of their profile, which could be good for the company as it indicates higher interest."""

# Countplot for Last Activity
sns.countplot(x = df['last_activity'])
plt.show()

df['last_activity'].value_counts(normalize=True)

"""About half of the leads last interacted with ExtraaLearn through email, while the other half interacted through a phone or the website."""

# Countplot for Print Media Type 1
sns.countplot(x = df['print_media_type1'])
plt.show()

df['print_media_type1'].value_counts(normalize=True)

"""Most of the leads (i.e. about 89%) did not see the ExtraaLearn newspaper advertisement."""

# Countplot for Print Media Type 2
sns.countplot(x = df['print_media_type2'])
plt.show()

df['print_media_type2'].value_counts(normalize=True)

"""Most of the leads (i.e. about 95%) did not see the ExtraaLearn magazine advertisement."""

# Countplot for Digital Media
sns.countplot(x = df['digital_media'])
plt.show()

df['digital_media'].value_counts(normalize=True)

"""Most of the leads (i.e. about 89%) did not see the ExtraaLearn advertisements on the digital platforms."""

# Countplot for Educational Channels
sns.countplot(x = df['educational_channels'])
plt.show()

df['educational_channels'].value_counts(normalize=True)

"""Most of the leads (i.e. about 85%) did not hear about ExtraaLearn through educational channels."""

# Countplot for Referral
sns.countplot(x = df['referral'])
plt.show()

df['referral'].value_counts(normalize=True)

"""Almost all the leads (i.e. about 98%) did not hear about ExtraaLearn through referral."""

# Bivariate Analysis
# Defining the stacked_barplot() function
def stacked_barplot(data,predictor,target,figsize=(10,6)):
  (pd.crosstab(data[predictor],data[target],normalize='index')*100).plot(kind='bar',figsize=figsize,stacked=True)
  plt.legend(loc="lower right")
  plt.ylabel(target)

# Comparing Lead Status for various Current Occupations
stacked_barplot(df, 'current_occupation', 'status')

"""Leads with a professional occupation were converted to paid customers at the highest frequency, which makes sense as they are the group that most likely has the money to afford this service, while student leads became customers the least often, which could be due to them focusing on their education and possibly having student debt."""

# Comparing Lead Status for various methods of First Interaction
stacked_barplot(df, 'first_interaction', 'status')

"""The proportion of leads whose first interaction was through the website and became paid customers around 3-4 times greater than the proportion of leads whose first interaction was through the mobile app and became paid customers. This could be due to the website providing a more comprehensive user experience compared to the mobile app, thus doing a better job at attracting customers."""

# Comparing Lead Status for various Modes of Interaction
stacked_barplot(df, 'last_activity', 'status')

"""The proporiton of leads converted to paid customers based on their last interaction with ExtraaLearn is relatively consistent, with leads whose last interaction being through the website converting to paid customers at the highest frequency by a small margin and those who interacted through the phone converting at the lowest frequency. Once again, this could be due to a better user experience on the website, though the difference is smaller in this case."""

# Comparing Lead Status for various Channels
stacked_barplot(df, 'print_media_type1', 'status')

"""Regardless of whether leads viewed the ExtraaLearn newspaper ad, it seems that the proportion of leads converted to paid customers stayed almost the same, meaning the ad did not seem to have any visible influence on the leads' decisions."""

stacked_barplot(df, 'print_media_type2', 'status')

"""Regardless of whether leads viewed the ExtraaLearn magazine ad, it seems that the proportion of leads converted to paid customers stayed almost the same, meaning the ad did not seem to have any visible influence on the leads' decisions."""

stacked_barplot(df, 'digital_media', 'status')

"""Regardless of whether leads viewed the ExtraaLearn ad on digital platforms, it seems that the proportion of leads converted to paid customers stayed almost the same, meaning the ad did not seem to have any visible influence on the leads' decisions."""

stacked_barplot(df, 'educational_channels', 'status')

"""Regardless of whether leads heard about ExtraaLearn through educational channels, it seems that the proportion of leads converted to paid customers stayed almost the same, meaning the channels did not seem to have any visible influence on the leads' decisions."""

stacked_barplot(df, 'referral', 'status')

"""It seems that the proportion of leads that were referred to ExtraaLearn by another customers and converted to paid customers was about two times greater than the proportion of leads that were not referred and converted. This indicates that referrals could have a signficant influence on a lead's decision to become a customer or not, potentially due to the lead trusting the feedback given by their referral about ExtraaLearn because of a good relationship."""

# Comparing Lead Status for various stages of Profile Completion
stacked_barplot(df, 'profile_completed', 'status')

"""It seems that leads who completed 75%-100% of their profile (High) were converted to paid customers at the highest rate, while those who only completed 0%-50% of their profile were converted at the lowest rate. This could be due to those completing more of their profile having more interest in ExtraaLearn and thus being more likely to commit to being a customer.

## Data Preprocessing

- Missing value treatment (if needed)
- Feature engineering (if needed)
- Outlier detection and treatment (if needed)
- Preparing data for modeling
- Any other preprocessing steps (if needed)
"""

# No missing values in Data set as found previously

# Outlier treatment

# Dropping observaions with website visit counts greater than 9.5. There are 154 such observations
df.drop(index=df[df.website_visits>9.5].index,inplace=True)
# Dropping observaions with page views per visit counts greater than 6.274. There are 257 such observations
df.drop(index=df[df.page_views_per_visit>6.274].index,inplace=True)

#Prepare Data for Modeling

# Separating target variable and other variables
X=df.drop(columns='status')
Y=df['status']

# Splitting the data into train and test sets
X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.30,random_state=1,stratify=Y)

# Checking that no column has missing values in train or test sets
print(X_train.isna().sum())
print('-'*30)
print(X_test.isna().sum())

# Scaling the numerical data
sc=StandardScaler()

# Numerical Columns
num_col = ["age", "website_visits", "time_spent_on_website", "page_views_per_visit"]

# Fit_transform on train data
X_train_scaled = X_train.copy()
X_train_scaled[num_col]=sc.fit_transform(X_train[num_col])
X_train_scaled=pd.DataFrame(X_train_scaled, columns=X_train.columns)

# Transform on test data
X_test_scaled = X_test.copy()
X_test_scaled[num_col]=sc.transform(X_test[num_col])
X_test_scaled=pd.DataFrame(X_test_scaled, columns=X_test.columns)

# Converting Numerical Variables to type Float
for column in ['age', 'website_visits', 'time_spent_on_website', 'page_views_per_visit']:
    X_train[column]=X_train[column].astype('float')
    X_test[column]=X_test[column].astype('float')

# List of Categorical Variables
col_dummy = cat_col.copy()
#Encoding categorical variables
X_train_scaled=pd.get_dummies(X_train_scaled, columns=col_dummy, drop_first=True)
X_test_scaled=pd.get_dummies(X_test_scaled, columns=col_dummy, drop_first=True)

# Compiled Dataset with Scaled Numerical Variables and Dummy Categorical Variables to Use for EDA after Data Manipulation
df_scaled = pd.concat([X_test_scaled, X_train_scaled])
# Verify Data Preprocessing was Done Correctly
df_scaled.head()

"""## EDA

- It is a good idea to explore the data once again after manipulating it.

Since the categorical columns of data in the dataset were only encoded using dummy variables and thus convey the same information, only the 4 key numerical variables are necessary to explore after data preprocessing.
"""

# Histogram and Boxplot for Age
hist_box(df_scaled, "age")

# Histogram and Boxplot for Website Visits
hist_box(df_scaled, "website_visits")

# Histogram and Boxplot for Time Spent on Website
hist_box(df_scaled, "time_spent_on_website")

# Histogram and Boxplot for Page Views per Visit
hist_box(df_scaled, "page_views_per_visit")

"""It seems that after feature scaling, all four numerical variables retained the same distributional properties in terms of skew, spread, shape and peaks."""

# Heat map to show correlation between numerical variables
cols_list = ["age", "website_visits", "time_spent_on_website", "page_views_per_visit"]

plt.figure(figsize=(12, 7))
sns.heatmap(df_scaled[cols_list].corr(), annot=True, cmap='coolwarm')
plt.show()

"""All of the numerical variables in this dataset seem to have little to no correlation with each other, indicating minimal to no multicollinearity."""

# Compile X and y Data for Violin Plots
y_all = pd.concat([y_test, y_train])
df_scaled["status"] = y_all

# Violin Plots to Investigate Lead Age vs Lead Status
sns.violinplot(data = df_scaled, x = "status", y = 'age')
plt.title('Distribution of Age for Leads that did and did not Convert to Paid Customers')
plt.show()

"""The distribution of lead age seems to stay relatively consistent regardless of whether the lead was converted to a paid customer or not. It just appears that a slightly higher proportion of older leads were converted to paid customers rather than not."""

# Violin Plots to Investigate Lead Website Visits vs Lead Status
sns.violinplot(data = df_scaled, x = 'status', y = 'website_visits')
plt.title('Distribution of Website Visits for Leads that did and did not Convert to Paid Customers')
plt.show()

"""The distribution of lead website visits appears to stay relatively consistent regardless of whether or not the lead was converted to a paid customer."""

# Violin Plots to Investigate Lead Time Spent on Website vs Lead Status
sns.violinplot(data = df_scaled, x = 'status', y = 'time_spent_on_website')
plt.title('Distribution of Time Spent on Website for Leads that did and did not Convert to Paid Customers')
plt.show()

"""The violin plot for leads that were not converted to paid customers is considerable more dense for smaller time values, indicating that leads that were not converted to paid customers tended to spent less time on the website, whereas for converted leads, the distribution is more even."""

# Violin Plots to Investigate Lead Page Views Per Visit vs Lead Status
sns.violinplot(data = df_scaled, x = 'status', y = 'page_views_per_visit')
plt.title('Distribution of Page Views Per Visit for Leads that did and did not Convert to Paid Customers')
plt.show()

"""The distribution of lead page views per visit appears to stay relatively consistent regardless of whether or not the lead was converted to a paid customer.

## Building a Decision Tree model
"""

# Creating metric function
def metrics_score(actual, predicted):
    print(classification_report(actual, predicted))

    cm = confusion_matrix(actual, predicted)
    plt.figure(figsize=(8,5))

    sns.heatmap(cm, annot=True,  fmt='.2f')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()
#Create and Train Model
model_dt = DecisionTreeClassifier(random_state=1)
model_dt.fit(X_train_scaled, y_train)

# Checking performance on the training dataset
pred_train_dt = model_dt.predict(X_train_scaled)
metrics_score(y_train, pred_train_dt)

"""The decision tree model has performed extremely well on the training set with only 2 incorrectly classified data points, which could indicate overfitting."""

# Checking performance on the testing dataset
pred_test_dt = model_dt.predict(X_test_scaled)
metrics_score(y_test, pred_test_dt)

"""Given the considerable amount of mistakes on the test set (82% accuracy)
compared to the training set (nearly 100%), we can conclude that the model is overfitting and thus needs to be pruned.

## Do we need to prune the tree?
"""

#Hyperparameter tuning

# Choose the type of classifier.
estimator = DecisionTreeClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {
    "max_depth": np.arange(2, 7, 2),
    "max_leaf_nodes": [50, 75, 150, 250],
    "min_samples_split": [10, 30, 50, 70],
}


# Run the grid search
grid_obj = GridSearchCV(estimator, parameters, cv=5,scoring='recall',n_jobs=-1)
grid_obj = grid_obj.fit(X_train_scaled, y_train)

# Set the clf to the best combination of parameters
estimator = grid_obj.best_estimator_

# Fit the best algorithm to the data.
estimator.fit(X_train_scaled, y_train)

# Checking performance on the training dataset
dt_tuned = estimator.predict(X_train_scaled)
metrics_score(y_train, dt_tuned)

# Checking performance on the test dataset
y_pred_tuned = estimator.predict(X_test_scaled)
metrics_score(y_test, y_pred_tuned)

"""With the default parameters, the decision tree model overfits the training data and fails to generalize to the test data, but the pruned model is able to do so with an 85% accuracy on the training set and 83% accuracy on the test set. However, there is likely still some overfitting present because the test data performance did not improve significantly.

More specifically the recall, which is the priority in this context, of the tuned model slightly decreased from training to testing. Ideally, ExtraaLearn wants to identify all leads who are likely to convert to paid customers. It is more costly for them to miss potential customers (false negatives) compared to incorrectly labeling a non-converting lead as likely to convert (false positives). A high recall ensures that the model captures as many leads who will actually convert as possible.

Practically, if the model has a low recall, ExtraaLearn might fail to engage with potentially convertible leads, resulting in lost revenue. However, a false positive results in wasted effort on leads that don’t convert, which has less of an impact than missing a lead who would have become a customer.

Thus, it is crucial that the recall of this model be optimized through further tuning.
"""

# Visualize Decision Tree
feature_names = list(X_train_scaled.columns)
plt.figure(figsize=(20, 10))
out = tree.plot_tree(
    estimator,
    max_depth=4,
    feature_names=feature_names,
    filled=True,
    fontsize=9,
    node_ids=False,
    class_names=None,
)
# below code will add arrows to the decision tree split if they are missing
for o in out:
    arrow = o.arrow_patch
    if arrow is not None:
        arrow.set_edgecolor("black")
        arrow.set_linewidth(1)
plt.show()

# Importance of features in the tree building

importances = estimator.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(8, 8))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""We see that the most importance features are the lead's first interaction with ExtraaLearn being through the website, the time the lead spent on the website, and whether or not the lead completed at least 50% of their profile (Medium).

## Building a Random Forest model
"""

#Create and Train Model
rf_estimator = RandomForestClassifier(random_state = 1)

rf_estimator.fit(X_train_scaled, y_train)

# Checking performance on the training dataset
y_pred_train_rf = rf_estimator.predict(X_train_scaled)

metrics_score(y_train, y_pred_train_rf)

"""The random forest model has performed extremely well on the training set with only 2 incorrectly classified data points, which could indicate overfitting."""

# Checking performance on the test dataset
y_pred_test_rf = rf_estimator.predict(X_test_scaled)

metrics_score(y_test, y_pred_test_rf)

"""Given the considerable amount of mistakes on the test set (86% accuracy) compared to the training set (nearly 100% accuracy), we can conclude that the model is overfitting and thus needs to be pruned.

## Do we need to prune the tree?
"""

# Choose the type of classifier.
rf_estimator_tuned = RandomForestClassifier(random_state=1)

# Grid of parameters to choose from
parameters = {"n_estimators": [100,150,200],
    "max_depth": [4, 6, 8],
    "min_samples_leaf": [20, 40, 60],
    "min_samples_split": [50, 75, 100]
             }

# Run the grid search
grid_obj = GridSearchCV(rf_estimator_tuned, parameters, cv=5, scoring='recall',n_jobs=-1)
grid_obj = grid_obj.fit(X_train_scaled, y_train)

# Set the clf to the best combination of parameters
rf_estimator_tuned = grid_obj.best_estimator_

# Fit the best algorithm to the data.
rf_estimator_tuned.fit(X_train_scaled, y_train)

# Checking performance on the training dataset
rf_train_tuned = rf_estimator_tuned.predict(X_train_scaled)
metrics_score(y_train, rf_train_tuned)

# Checking performance on the test dataset
rf_test_tuned = rf_estimator_tuned.predict(X_test_scaled)
metrics_score(y_test, rf_test_tuned)

"""With the default parameters, the random forest model overfits the training data and fails to generalize to the test data, but the pruned model is able to do so. However, there is likely still some overfitting present because the test data performance did not improve between the default and tuned models, as both had 86% accuracy.

As stated before, a high recall is the priority for ExtraaLearn, thus it is not ideal that the precision is higher than the recall for this tuned model. Further tuning would be needed to optimize the recall while not completely sacrificing precision.
"""

# Check the most important features
importances = rf_estimator_tuned.feature_importances_

columns = X_train_scaled.columns

importance_df = pd.DataFrame(importances, index = columns, columns = ['Importance']).sort_values(by = 'Importance', ascending = False)

plt.figure(figsize = (13, 13))

sns.barplot(x = importance_df.Importance, y = importance_df.index, color="violet")

"""We see that the tuned random forest model considered the same 3 features as the most important among those in the dataset that the decision tree model did (website first interaction, time spent on website, and medium completed profile).

## Actionable Insights and Recommendations

Since leads who first interacted through the website converted at a significantly higher rate than those who used the mobile app, ExtraaLearn should work on optimizing its website experience. Also, leads who spent more time on the website showed a higher chance of conversion, emphasizing the importance of engagement and user experience. Leads who completed 75%-100% of their profile were much more likely to convert to paid customers compared to leads with lower profile completion, thus they should be encouraged to complete more of their profiles, possibly though progress reminders, gamification, or incentives.

Professionals are the most likely to convert to paid customers, likely due to having the resources and motivation to invest in ExtraaLearn, while students converted at the lowest rate, possibly due to financial constraints. ExtraaLearn should tailor its marketing efforts towards professionals, offering more value propositions that resonate with their career goals. Student discounts and university-tailored opportunities could make the programs more accessible and appealing to students, thus increasing their conversion rates.

With leads referred by existing customers converting at twice the rate of non-referred leads, ExtraaLearn should expand or enhance its referral programs with incentives, for instance providing cash rewards for customers that successfully refer a new lead that becomes a customer. In contrast, traditional advertising methods such as newspaper, magazine, and digital ads showed little influence on conversion rates, suggesting ExtraaLearn should consider reallocating funds from these channels to more direct engagement strategies like personalized email marketing, webinars, or targeted content.

Across all lead interactions, conversion rates were relatively consistent whether the last interaction occurred via email, phone, or website, though website interactions slightly outperformed other channels. This highlights the importance of maintaining a multi-channel approach but suggests ExtraaLearn could prioritize website-driven follow-ups for better conversion potential.

In terms of model performance, both the decision tree and random forest models showed signs of overfitting, particularly with training set accuracy nearing 100%, while test set accuracy was lower (around 83-86%). To address this, ExtraaLearn should consider additional pruning of the models or experimenting with alternative algorithms like logistic regression or gradient boosting, which may better generalize to new data and improve the recall of the model. By focusing on these areas, ExtraaLearn can significantly enhance its lead conversion process and achieve better customer acquisition outcomes.
"""